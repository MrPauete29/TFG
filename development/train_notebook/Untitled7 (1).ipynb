{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExTunGdVU9sU"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "m_d3KhrQUvQf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from typing import Tuple, Any, Dict, List\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def factorize(df: pd.DataFrame, columns: List = None) -> pd.DataFrame:\n",
        "    if columns is None:\n",
        "        for column in df.columns:\n",
        "            if df[column].dtype not in [\"int64\",\"float64\"]:\n",
        "                df[column] = df[column].factorize()[0]\n",
        "    else:\n",
        "        for column in columns:\n",
        "            df[column] = df[column].factorize()[0]\n",
        "    return df\n",
        "\n",
        "def eliminate_column_na(df: pd.DataFrame, threshold: int) -> pd.DataFrame:\n",
        "    threshold = threshold / 100\n",
        "    threshold = threshold * len(df)\n",
        "    df = df.dropna(axis=1, thresh=int(threshold))\n",
        "    return df\n",
        "\n",
        "def eliminate_row_na(df: pd.DataFrame, threshold: int) -> pd.DataFrame:\n",
        "    threshold = 100 - threshold\n",
        "    threshold = threshold / 100\n",
        "    threshold = threshold * len(df.columns)\n",
        "    df = df.dropna(axis=0, thresh=int(threshold))\n",
        "    return df\n",
        "\n",
        "def split_dataset(df: pd.DataFrame,target_column: str) -> (pd.DataFrame,pd.DataFrame):\n",
        "    target_df = df[target_column]\n",
        "    features_df = df.drop(columns=[target_column])\n",
        "    return features_df,target_df\n",
        "\n",
        "def join_dataframe_columns(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df1.join(df2)\n",
        "\n",
        "def join_dataframe_rows(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
        "    return pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "def read_csv_file(file) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(file)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"Archivo no encontrado\")\n",
        "    except:\n",
        "        raise ValueError(\"Error en la lectura del archivo\")\n",
        "\n",
        "def write_csv_file(df: pd.DataFrame, nombre_archivo: str = None, ruta: str = None):\n",
        "    if nombre_archivo is None:\n",
        "        raise TypeError(\"Nombre de archivo no especificado\")\n",
        "    if ruta is None:\n",
        "        df.to_csv(nombre_archivo, index = False)\n",
        "        return\n",
        "    df.to_csv(f\"{ruta}/{nombre_archivo}\")\n",
        "\n",
        "def train_test_validation_split(df: pd.DataFrame, target_column: str, test_size: float = 0.1, validation_size: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    X, y = split_dataset(df, target_column)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size + validation_size, random_state=19)\n",
        "    validation_size_proportioned = validation_size / (test_size + validation_size)\n",
        "    X_test, X_validation, y_test, y_validation = train_test_split(X_temp, y_temp, test_size=validation_size_proportioned, random_state=19)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, X_validation, y_validation\n",
        "def model_best_parameters(model: Any, scoring: str = \"recall\", param_distributions: Dict = None ):\n",
        "    print(scoring)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=1,\n",
        "        cv=2,\n",
        "        scoring=scoring,\n",
        "        n_jobs=1,\n",
        "        verbose=2,\n",
        "        random_state=42\n",
        "    )\n",
        "    return random_search\n",
        "def create_visualize_confusion_matrix(y_true: Any, y_pred: Any) -> None:\n",
        "    cm = confusion_matrix(y_true,y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "def create_visualize_classification_report(y_true: Any,y_pred: Any) -> None:\n",
        "    report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
        "    df_report = pd.DataFrame(report_dict).transpose()\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.heatmap(df_report.iloc[:-3, :-1], annot=True, cmap='Blues')\n",
        "    plt.title('Classification Report Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocLTBr8lVDIY"
      },
      "source": [
        "# Clean Stroke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "5HQuvFGBUwbX"
      },
      "outputs": [],
      "source": [
        "def clean_stroke(df):\n",
        "    smoke_map = {\n",
        "    \"never smoked\": 0,\n",
        "    \"formerly smoked\": 1,\n",
        "    \"smokes\": 2,\n",
        "    \"Unknown\": -1\n",
        "    }\n",
        "    married_map = {\n",
        "        \"Yes\": 1,\n",
        "        \"No\": 0\n",
        "    }\n",
        "\n",
        "    df = df.drop(\"id\", axis = 1)\n",
        "    df[\"bmi\"] = df[\"bmi\"].fillna(df[\"bmi\"].median())\n",
        "    df[\"ever_married\"] = df[\"ever_married\"].map(married_map)\n",
        "    df[\"smoking_status\"] = df[\"smoking_status\"].map(smoke_map)\n",
        "    df = factorize(df, [\"gender\", \"Residence_type\",\"work_type\"])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKynbFLIVHmO"
      },
      "source": [
        "# Clean Heart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "RqskFaJhUwpr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_heart(df):\n",
        "    map_edades = {\n",
        "        \"18-24\": 0,\n",
        "        \"25-29\": 0,\n",
        "        \"30-34\": 0,\n",
        "        \"35-39\": 1,\n",
        "        \"40-44\": 1,\n",
        "        \"45-49\": 1,\n",
        "        \"50-54\": 2,\n",
        "        \"55-59\": 2,\n",
        "        \"60-64\": 2,\n",
        "        \"65-69\": 3,\n",
        "        \"70-74\": 3,\n",
        "        \"75-79\": 3,\n",
        "        \"80 or older\": 3\n",
        "    }\n",
        "    map_diabetes = {\n",
        "        \"No\": 0,\n",
        "        \"No, borderline diabetes\": 1,\n",
        "        \"Yes (during pregnancy)\": 2,\n",
        "        \"Yes\": 3\n",
        "    }\n",
        "    map_genhealth = {\n",
        "        \"Excellent\": 5,\n",
        "        \"Very good\": 4,\n",
        "        \"Good\": 3,\n",
        "        \"Fair\": 2,\n",
        "        \"Poor\": 1\n",
        "    }\n",
        "    map_yes_no = {\n",
        "        \"Yes\": 1,\n",
        "        \"No\": 0\n",
        "    }\n",
        "    columns = [\"Sex\", \"Race\"]\n",
        "    df = eliminate_column_na(df, 30)\n",
        "    df = eliminate_row_na(df, 20)\n",
        "    df = df.drop([\"PhysicalHealth\", \"MentalHealth\"], axis=1)\n",
        "    df[\"AgeCategory\"] = df[\"AgeCategory\"].map(map_edades)\n",
        "    df[\"Diabetic\"] = df[\"Diabetic\"].map(map_diabetes)\n",
        "    df[\"GenHealth\"] = df[\"GenHealth\"].map(map_genhealth)\n",
        "    df[\"Smoking\"] = df[\"Smoking\"].map(map_yes_no)\n",
        "    df[\"AlcoholDrinking\"] = df[\"AlcoholDrinking\"].map(map_yes_no)\n",
        "    df[\"HeartDisease\"] = df[\"HeartDisease\"].map(map_yes_no)\n",
        "    df[\"Stroke\"] = df[\"Stroke\"].map(map_yes_no)\n",
        "    df[\"SkinCancer\"] = df[\"SkinCancer\"].map(map_yes_no)\n",
        "    df[\"KidneyDisease\"] = df[\"KidneyDisease\"].map(map_yes_no)\n",
        "    df[\"Asthma\"] = df[\"Asthma\"].map(map_yes_no)\n",
        "    df[\"DiffWalking\"] = df[\"DiffWalking\"].map(map_yes_no)\n",
        "    df = factorize(df, columns)\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDXZXDlGVuFG"
      },
      "source": [
        "# Técnicas de Balanceo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "oQSBQ0MRUw5k"
      },
      "outputs": [],
      "source": [
        "def SMOTE_technique(X_train, y_train):\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    smote = SMOTE(random_state=50)\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "    return X_train, y_train\n",
        "def under_sampling_technique(X_train, y_train):\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "    rus = RandomUnderSampler(sampling_strategy='auto', random_state=50)\n",
        "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "    return X_resampled, y_resampled\n",
        "def over_sampling_technique(X_train, y_train):\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=50)\n",
        "    X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "    return X_train, y_train\n",
        "def smoteen_technique(X_train, y_train):\n",
        "    from imblearn.combine import SMOTEENN\n",
        "    smote_enn = SMOTEENN(random_state=50)\n",
        "    X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
        "    return X_train, y_train\n",
        "def smotetomek_technique(X_train, y_train):\n",
        "    from imblearn.combine import SMOTETomek\n",
        "    smote_tomek = SMOTETomek(random_state=50)\n",
        "    X_train, y_train = smote_tomek.fit_resample(X_train, y_train)\n",
        "    return X_train, y_train\n",
        "def tomek_links_technique(X_train, y_train):\n",
        "    from imblearn.under_sampling import TomekLinks\n",
        "    tomek = TomekLinks(sampling_strategy='auto')\n",
        "    X_resampled, y_resampled = tomek.fit_resample(X_train, y_train)\n",
        "    return X_resampled, y_resampled\n",
        "def tomek_links_smote_technique(X_train, y_train):\n",
        "    from imblearn.combine import SMOTETomek\n",
        "    smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=50)\n",
        "    X_train, y_train = smote_tomek.fit_resample(X_train, y_train)\n",
        "    return X_train, y_train\n",
        "def nothing(X_train, y_train):\n",
        "    return X_train, y_train\n",
        "lista_funciones = [under_sampling_technique, over_sampling_technique, SMOTE_technique, tomek_links_technique, tomek_links_smote_technique,nothing]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-qQtL09V8qT"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "YTOLLBSgUxHD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression as lr\n",
        "from sklearn.metrics import classification_report, make_scorer, recall_score, fbeta_score,f1_score\n",
        "import joblib\n",
        "from typing import Dict\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, pos_label):\n",
        "        self.pos_label = pos_label\n",
        "        self.model = lr()\n",
        "        self.trained = False\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame, param_distributions: Dict ):\n",
        "        self.trained = True\n",
        "        scoring = make_scorer(f1_score, pos_label=self.pos_label)\n",
        "        self.model = model_best_parameters(self.model, scoring = scoring, param_distributions=param_distributions)\n",
        "        self.model.fit(X_train, y_train)\n",
        "        self.model = self.model.best_estimator_\n",
        "\n",
        "    def predict(self, X_val):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        return self.model.predict(X_val)\n",
        "\n",
        "    def predict_proba(self, X_val):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        return self.model.predict_proba(X_val)\n",
        "\n",
        "    def evaluate(self, X: pd.DataFrame = None, y_true: pd.DataFrame = None, y_pred: pd.DataFrame = None):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        if y_true is None:\n",
        "            raise ValueError(\"y set must be provided\")\n",
        "        if y_pred is None:\n",
        "            if X is None:\n",
        "                raise ValueError(\"y_pred set or X set must be provided\")\n",
        "            print(\"Prediciendo probabilidades...\")\n",
        "            y_proba = self.predict_proba(X)[:, 1]\n",
        "            precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "            best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "            y_pred = (y_proba > best_threshold).astype(int)\n",
        "        print(classification_report(y_true, y_pred))\n",
        "        # create_visualize_confusion_matrix(y_true, y_pred)\n",
        "        # create_visualize_classification_report(y_true, y_pred)\n",
        "\n",
        "    def feature_importance(self, feature_names: pd.Index):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "\n",
        "        coef = self.model.coef_[0]\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Coefficient': coef,\n",
        "            'Abs_Coefficient': np.abs(coef)\n",
        "        }).sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "        return importance_df[['Feature', 'Coefficient']]\n",
        "    def save_model(self, filename: str):\n",
        "        joblib.dump(self.model,f\"trained_models/random_forest/{filename}.pkl\")\n",
        "\n",
        "    def load_model(self, filename: str):\n",
        "        self.model = joblib.load(f\"trained_models/random_forest/{filename}.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ05FA6WWA2J"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "d5VNCV3dUxVt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, make_scorer, recall_score, fbeta_score,f1_score\n",
        "import joblib\n",
        "from typing import Dict\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, pos_label):\n",
        "        self.pos_label = pos_label\n",
        "        self.model = RandomForestClassifier(random_state = 50)\n",
        "        self.trained = False\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame, param_distributions: Dict):\n",
        "        self.trained = True\n",
        "        scoring = make_scorer(f1_score,pos_label=self.pos_label)\n",
        "        self.model = model_best_parameters(self.model,scoring = scoring, param_distributions = param_distributions)\n",
        "        self.model.fit(X_train, y_train)\n",
        "        self.model = self.model.best_estimator_\n",
        "    def predict(self, X_val):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        return self.model.predict(X_val)\n",
        "\n",
        "    def predict_proba(self, X_val):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        return self.model.predict_proba(X_val)\n",
        "\n",
        "    def evaluate(self, X: pd.DataFrame = None, y_true: pd.DataFrame = None, y_pred: pd.DataFrame = None):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        if y_true is None:\n",
        "            raise ValueError(\"y set must be provided\")\n",
        "        if y_pred is None:\n",
        "            if X is None:\n",
        "                raise ValueError(\"y_pred set or X set must be provided\")\n",
        "            print(\"Prediciendo probabilidades...\")\n",
        "            y_proba = self.predict_proba(X)[:, 1]\n",
        "            precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "            best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "            y_pred = (y_proba > best_threshold).astype(int)\n",
        "        # create_visualize_confusion_matrix(y_true, y_pred)\n",
        "        # create_visualize_classification_report(y_true,y_pred)\n",
        "        print(classification_report(y_true, y_pred))\n",
        "    def feature_importance(self, feature_names: pd.Index):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "\n",
        "        importances = self.model.feature_importances_\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importances\n",
        "        }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "        return importance_df\n",
        "    def save_model(self, filename: str) -> None:\n",
        "        joblib.dump(self.model,f\"trained_models/random_forest/{filename}.pkl\")\n",
        "\n",
        "    def load_model(self, filename: str) -> None:\n",
        "        self.model = joblib.load(f\"trained_models/random_forest/{filename}.pkl\")\n",
        "        self.trained = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD5XUaIzWDr7"
      },
      "source": [
        "#XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "94PJ15oXUxpK"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, make_scorer, recall_score, f1_score\n",
        "from typing import Dict\n",
        "import joblib\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "class XGBoost:\n",
        "    def __init__(self, pos_label):\n",
        "        self.pos_label = pos_label\n",
        "        self.model = xgb.XGBClassifier(random_state = 50)\n",
        "        self.trained = False\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame, param_distributions: Dict):\n",
        "        self.trained = True\n",
        "        scoring = make_scorer(f1_score, average = \"binary\")\n",
        "        self.model = model_best_parameters(self.model, scoring=scoring, param_distributions=param_distributions)\n",
        "        self.model.fit(X_train, y_train)\n",
        "        self.model = self.model.best_estimator_\n",
        "    def predict(self, X_val):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        return self.model.predict(X_val)\n",
        "\n",
        "    def predict_proba(self, X_val):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        return self.model.predict_proba(X_val)\n",
        "\n",
        "    def evaluate(self, X: pd.DataFrame = None, y_true: pd.DataFrame = None, y_pred: pd.DataFrame = None):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "        if y_true is None:\n",
        "            raise ValueError(\"y set must be provided\")\n",
        "        if y_pred is None:\n",
        "            if X is None:\n",
        "                raise ValueError(\"y_pred set or X set must be provided\")\n",
        "            print(\"Prediciendo probabilidades...\")\n",
        "            y_proba = self.predict_proba(X)[:, 1]\n",
        "            precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "            best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "            y_pred = (y_proba > best_threshold).astype(int)\n",
        "        print(classification_report(y_true, y_pred))\n",
        "        # create_visualize_confusion_matrix(y_true, y_pred)\n",
        "        # create_visualize_classification_report(y_true, y_pred)\n",
        "\n",
        "    def feature_importance(self, feature_names: pd.Index):\n",
        "        if not self.trained:\n",
        "            raise Exception(\"Model not trained\")\n",
        "\n",
        "        importances = self.model.feature_importances_\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importances\n",
        "        }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "        return importance_df\n",
        "    def save_model(self, filename: str) -> None:\n",
        "        joblib.dump(self.model,f\"trained_models/random_forest/{filename}.pkl\")\n",
        "\n",
        "    def load_model(self, filename: str) -> None:\n",
        "        self.model = joblib.load(f\"trained_models/random_forest/{filename}.pkl\")\n",
        "        self.trained = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkwNswDhYlFQ"
      },
      "source": [
        "#Entrenamiento y evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "C57zL2HLfApr"
      },
      "outputs": [],
      "source": [
        "param_distributions_logistic = [\n",
        "    {\n",
        "        'penalty': ['l1'],\n",
        "        'solver': ['liblinear'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'max_iter': [100, 200, 300, 500]\n",
        "    },\n",
        "    {\n",
        "        'penalty': ['l2'],\n",
        "        'solver': ['liblinear', 'saga'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'max_iter': [100, 200, 300, 500]\n",
        "    },\n",
        "    {\n",
        "        'penalty': ['elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0],\n",
        "        'max_iter': [100, 200, 300, 500]\n",
        "    }\n",
        "]\n",
        "param_distributions_random_forest = {\n",
        "        \"n_estimators\": [100, 300, 500, 700, 1000],\n",
        "        \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
        "        \"min_samples_split\": [2, 5, 10],\n",
        "        \"min_samples_leaf\": [1, 2, 4],\n",
        "        \"max_features\": [\"sqrt\", \"log2\"],\n",
        "        \"bootstrap\": [True]\n",
        "    }\n",
        "param_distributions_xgboost = {\n",
        "        \"n_estimators\": [100, 300, 500, 700],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
        "        \"max_depth\": [3, 5, 7, 10],\n",
        "        \"subsample\": [0.6, 0.8, 1.0],\n",
        "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "        \"gamma\": [0, 1, 3, 5],\n",
        "        \"reg_alpha\": [0, 0.1, 0.5, 1],\n",
        "        \"reg_lambda\": [0.5, 1, 2]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q9lsZdfYkI1",
        "outputId": "ccecd938-b92e-460d-b4e6-d50a91c05c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: under_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       972\n",
            "           1       0.22      0.48      0.30        50\n",
            "\n",
            "    accuracy                           0.89      1022\n",
            "   macro avg       0.59      0.70      0.62      1022\n",
            "weighted avg       0.93      0.89      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: under_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.3s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.3s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95       972\n",
            "           1       0.22      0.34      0.26        50\n",
            "\n",
            "    accuracy                           0.91      1022\n",
            "   macro avg       0.59      0.64      0.61      1022\n",
            "weighted avg       0.93      0.91      0.92      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: under_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.89      0.93       972\n",
            "           1       0.20      0.50      0.28        50\n",
            "\n",
            "    accuracy                           0.87      1022\n",
            "   macro avg       0.58      0.70      0.61      1022\n",
            "weighted avg       0.93      0.87      0.90      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: over_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93       972\n",
            "           1       0.21      0.60      0.31        50\n",
            "\n",
            "    accuracy                           0.87      1022\n",
            "   macro avg       0.59      0.74      0.62      1022\n",
            "weighted avg       0.94      0.87      0.90      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: over_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.0s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.84      0.90       972\n",
            "           1       0.17      0.66      0.27        50\n",
            "\n",
            "    accuracy                           0.83      1022\n",
            "   macro avg       0.58      0.75      0.59      1022\n",
            "weighted avg       0.94      0.83      0.87      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: over_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93       972\n",
            "           1       0.20      0.56      0.29        50\n",
            "\n",
            "    accuracy                           0.87      1022\n",
            "   macro avg       0.59      0.72      0.61      1022\n",
            "weighted avg       0.94      0.87      0.90      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: SMOTE_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92       972\n",
            "           1       0.20      0.64      0.31        50\n",
            "\n",
            "    accuracy                           0.86      1022\n",
            "   macro avg       0.59      0.76      0.62      1022\n",
            "weighted avg       0.94      0.86      0.89      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: SMOTE_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.4s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.4s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       972\n",
            "           1       0.20      0.42      0.27        50\n",
            "\n",
            "    accuracy                           0.89      1022\n",
            "   macro avg       0.58      0.67      0.61      1022\n",
            "weighted avg       0.93      0.89      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: SMOTE_technique\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       972\n",
            "           1       0.22      0.48      0.30        50\n",
            "\n",
            "    accuracy                           0.89      1022\n",
            "   macro avg       0.59      0.70      0.62      1022\n",
            "weighted avg       0.93      0.89      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: tomek_links_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92       972\n",
            "           1       0.20      0.66      0.31        50\n",
            "\n",
            "    accuracy                           0.86      1022\n",
            "   macro avg       0.59      0.76      0.61      1022\n",
            "weighted avg       0.94      0.86      0.89      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: tomek_links_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.6s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.6s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93       972\n",
            "           1       0.22      0.62      0.33        50\n",
            "\n",
            "    accuracy                           0.87      1022\n",
            "   macro avg       0.60      0.75      0.63      1022\n",
            "weighted avg       0.94      0.87      0.90      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: tomek_links_technique\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       972\n",
            "           1       0.22      0.48      0.30        50\n",
            "\n",
            "    accuracy                           0.89      1022\n",
            "   macro avg       0.60      0.70      0.62      1022\n",
            "weighted avg       0.93      0.89      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: tomek_links_smote_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92       972\n",
            "           1       0.20      0.66      0.31        50\n",
            "\n",
            "    accuracy                           0.86      1022\n",
            "   macro avg       0.59      0.76      0.61      1022\n",
            "weighted avg       0.94      0.86      0.89      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: tomek_links_smote_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.4s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.4s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.94       972\n",
            "           1       0.20      0.46      0.28        50\n",
            "\n",
            "    accuracy                           0.88      1022\n",
            "   macro avg       0.58      0.68      0.61      1022\n",
            "weighted avg       0.93      0.88      0.90      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: tomek_links_smote_technique\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94       972\n",
            "           1       0.23      0.46      0.30        50\n",
            "\n",
            "    accuracy                           0.90      1022\n",
            "   macro avg       0.60      0.69      0.62      1022\n",
            "weighted avg       0.93      0.90      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: nothing\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92       972\n",
            "           1       0.20      0.66      0.31        50\n",
            "\n",
            "    accuracy                           0.86      1022\n",
            "   macro avg       0.59      0.76      0.62      1022\n",
            "weighted avg       0.94      0.86      0.89      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: nothing\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.6s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   0.6s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94       972\n",
            "           1       0.22      0.56      0.32        50\n",
            "\n",
            "    accuracy                           0.88      1022\n",
            "   macro avg       0.60      0.73      0.63      1022\n",
            "weighted avg       0.94      0.88      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: nothing\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       972\n",
            "           1       0.22      0.50      0.30        50\n",
            "\n",
            "    accuracy                           0.89      1022\n",
            "   macro avg       0.60      0.70      0.62      1022\n",
            "weighted avg       0.94      0.89      0.91      1022\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\EnfermedadCorazon.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: under_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.88      0.91     58484\n",
            "           1       0.30      0.56      0.39      5475\n",
            "\n",
            "    accuracy                           0.85     63959\n",
            "   macro avg       0.63      0.72      0.65     63959\n",
            "weighted avg       0.90      0.85      0.87     63959\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\EnfermedadCorazon.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: under_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   3.7s\n",
            "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   3.7s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.89      0.92     58484\n",
            "           1       0.31      0.53      0.39      5475\n",
            "\n",
            "    accuracy                           0.86     63959\n",
            "   macro avg       0.63      0.71      0.66     63959\n",
            "weighted avg       0.90      0.86      0.88     63959\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\EnfermedadCorazon.csv\n",
            "Modelo: XGBoost\n",
            "Función de muestreo: under_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', average=binary)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "[CV] END colsample_bytree=1.0, gamma=3, learning_rate=0.05, max_depth=3, n_estimators=300, reg_alpha=0, reg_lambda=0.5, subsample=1.0; total time=   0.0s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.89      0.92     58484\n",
            "           1       0.31      0.54      0.40      5475\n",
            "\n",
            "    accuracy                           0.86     63959\n",
            "   macro avg       0.63      0.72      0.66     63959\n",
            "weighted avg       0.90      0.86      0.88     63959\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\EnfermedadCorazon.csv\n",
            "Modelo: LogisticRegression\n",
            "Función de muestreo: over_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.9s\n",
            "[CV] END C=0.01, l1_ratio=0.5, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.9s\n",
            "Prediciendo probabilidades...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.88      0.92     58484\n",
            "           1       0.30      0.56      0.39      5475\n",
            "\n",
            "    accuracy                           0.85     63959\n",
            "   macro avg       0.63      0.72      0.66     63959\n",
            "weighted avg       0.90      0.85      0.87     63959\n",
            "\n",
            "Dataset: C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\EnfermedadCorazon.csv\n",
            "Modelo: RandomForest\n",
            "Función de muestreo: over_sampling_technique\n",
            "make_scorer(f1_score, response_method='predict', pos_label=1)\n",
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from typing import Tuple\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "for dataset, clean, target in zip([r\"C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\Stroke.csv\", r\"C:\\Users\\paumo\\PycharmProjects\\TFG\\datasets\\EnfermedadCorazon.csv\"],[clean_stroke, clean_heart],[\"stroke\",\"HeartDisease\"]):\n",
        "    df = read_csv_file(dataset)\n",
        "    df = clean(df)\n",
        "    X = df.drop(target, axis=1)\n",
        "    y = df[target]\n",
        "    cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "        (\"cat\", OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features)\n",
        "    ])\n",
        "    X_encoded = preprocessor.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, stratify=y, test_size=0.2, random_state=50)\n",
        "    for funcion in lista_funciones:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, stratify=y, test_size=0.2, random_state=50)\n",
        "        X_train, y_train = funcion(X_train, y_train)\n",
        "        weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "        class_weights = dict(zip(np.unique(y_train), weights))\n",
        "        for model, param_distribution in zip([LogisticRegression(pos_label=1), RandomForest(pos_label=1), XGBoost(pos_label=1)],[param_distributions_logistic,param_distributions_random_forest, param_distributions_xgboost]):\n",
        "            print(f\"Dataset: {dataset}\")\n",
        "            print(f\"Modelo: {model.__class__.__name__}\")\n",
        "            print(f\"Función de muestreo: {funcion.__name__}\")\n",
        "            model.train(X_train, y_train, param_distribution)\n",
        "            model.evaluate(X_test, y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eTGSUGeUx35"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIINcEDhUyIf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA93lRkdUybF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
